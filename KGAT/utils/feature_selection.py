import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ
file_path = "wine_info_processed_quintiles.csv"
df = pd.read_csv(file_path, dtype=str)  # ëª¨ë“  ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë¶ˆëŸ¬ì˜¤ê¸° (íƒ€ì… ë¬¸ì œ ë°©ì§€)
df = df.drop(columns=['URL'], errors='ignore')
df = df.drop(columns=['Wine Name'], errors='ignore')


# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ê²°ì¸¡ê°’ì„ "Unknown"ìœ¼ë¡œ ëŒ€ì²´)
df.fillna("Unknown", inplace=True)

# ëª¨ë“  ë¬¸ìì—´ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜
label_encoders = {}
for col in df.columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # ê° ì»¬ëŸ¼ë³„ ì¸ì½”ë” ì €ì¥

# íƒ€ê²Ÿ ë³€ìˆ˜ ì„ íƒ (ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ ì„ íƒ)
target_col = "Average Rating"  # ì˜ˆì œì—ì„œëŠ” ê°€ê²©ì„ ê¸°ì¤€ìœ¼ë¡œ ë³€ìˆ˜ ì¤‘ìš”ë„ í‰ê°€
X = df.drop(columns=[target_col])
y = df[target_col]

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# ë³€ìˆ˜ ì¤‘ìš”ë„ ì¶”ì¶œ
feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': rf.feature_importances_})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# ì¤‘ìš”ë„ ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'], color='skyblue')
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Feature Importance using Random Forest")
plt.gca().invert_yaxis()
plt.show()

# ìƒìœ„ 10ê°œ ë³€ìˆ˜ ì¶œë ¥
print("Top 10 Important Features:")
print(feature_importances.head(10))


import seaborn as sns
import matplotlib.pyplot as plt

# ì†ì„± ê°„ ìƒê´€ê´€ê³„ ê³„ì‚°
corr_matrix = df.corr()

# ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µ ì¶œë ¥
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity
import chardet


# ğŸ“Œ 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ
with open("wine_info_processed_quintiles.csv", "rb") as f:
    result = chardet.detect(f.read(100000))  # ì²˜ìŒ 100000ë°”ì´íŠ¸ë§Œ ì½ì–´ì„œ ê°ì§€
    detected_encoding = result["encoding"]
    print(f"âœ… ê°ì§€ëœ íŒŒì¼ ì¸ì½”ë”©: {detected_encoding}")

# ğŸ“Œ ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸°
wine_info = pd.read_csv("wine_info_processed_quintiles.csv", encoding=detected_encoding, encoding_errors="replace")

# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° (URL ë“±)
drop_cols = ["Wine Name", "URL", "Source File", "Search Page Link", "Actual Page Link"]
wine_info = wine_info.drop(columns=[col for col in drop_cols if col in wine_info.columns], errors="ignore")

# ğŸ“Œ 2ï¸âƒ£ ìƒê´€ê´€ê³„ ë¶„ì„ (ì¤‘ë³µ ì†ì„± ì œê±°)
print("ğŸ“¢ ìƒê´€ê´€ê³„ ë¶„ì„ ì§„í–‰ ì¤‘...")

wine_info_encoded = wine_info.copy()
encoder = LabelEncoder()

for col in wine_info_encoded.columns:
    wine_info_encoded[col] = encoder.fit_transform(wine_info_encoded[col].astype(str))

# ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ê³„ì‚°
corr_matrix = wine_info_encoded.corr()
high_corr_pairs = set()
to_remove = set()
selected_pairs = {}  # ì–´ë–¤ ì†ì„±ì´ ìœ ì§€ë˜ê³ , ì–´ë–¤ ì†ì„±ì´ ì œê±°ë˜ëŠ”ì§€ ì €ì¥

for col in corr_matrix.columns:
    for idx in corr_matrix.index:
        if col != idx and abs(corr_matrix.loc[idx, col]) > 0.85:
            # ì´ë¯¸ ì œê±°ëœ ì†ì„±ì´ë©´ ìŠ¤í‚µ
            if col in to_remove or idx in to_remove:
                continue
            
            # ë¬´ì¡°ê±´ í•˜ë‚˜ë§Œ ì œê±° (colì„ ê¸°ë³¸ì ìœ¼ë¡œ ì œê±°, idxë¥¼ ìœ ì§€)
            to_remove.add(idx)
            selected_pairs[idx] = col  # colì´ ì œê±°ë˜ê³ , idxê°€ ìœ ì§€ë¨

# ğŸ“¢ ì œê±°ë˜ëŠ” ë³€ìˆ˜ ì¶œë ¥ (ìœ ì§€ëœ ë³€ìˆ˜ë„ í•¨ê»˜ ì¶œë ¥)
print("ğŸš¨ ì œê±°ëœ ì†ì„± ëª©ë¡:")
for removed, kept in selected_pairs.items():
    print(f"âŒ {removed} ì œê±° (ğŸ”— {kept} ìœ ì§€, ìƒê´€ê³„ìˆ˜ {corr_matrix.loc[removed, kept]:.2f})")

# ì¤‘ë³µ ì†ì„± ì œê±°
wine_info_filtered = wine_info.drop(columns=to_remove)
print(f"âœ… ì œê±°ëœ ì¤‘ë³µ ì†ì„± ê°œìˆ˜: {len(to_remove)}")


# ğŸ“¢ ì œê±°ë˜ëŠ” ë³€ìˆ˜ ì¶œë ¥
print("ğŸš¨ ì œê±°ëœ ì†ì„± ëª©ë¡:")
for col1, col2 in high_corr_pairs:
    if col1 in to_remove:
        print(f"âŒ {col1} ì œê±° (ğŸ”— {col2}ì™€ ìƒê´€ê³„ìˆ˜ {corr_matrix.loc[col1, col2]:.2f})")

wine_info_filtered = wine_info.drop(columns=to_remove)
print(f"âœ… ì œê±°ëœ ì¤‘ë³µ ì†ì„± ê°œìˆ˜: {len(to_remove)}")


# ğŸ“Œ 3ï¸âƒ£ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê¸°ë°˜ ì†ì„± ì¤‘ìš”ë„ ë¶„ì„
print("ğŸ“¢ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê¸°ë°˜ ì†ì„± ì¤‘ìš”ë„ ë¶„ì„ ì¤‘...")
target_column = "Average Rating"
wine_info_filtered[target_column] = pd.to_numeric(wine_info_filtered[target_column], errors='coerce')
wine_info_filtered = wine_info_filtered.dropna(subset=[target_column])
wine_info_filtered[target_column] = wine_info_filtered[target_column].astype(float)

feature_columns = [col for col in wine_info_filtered.columns if col != target_column]

# ìˆ«ìë¡œ ë³€í™˜
for col in feature_columns:
    wine_info_filtered[col] = encoder.fit_transform(wine_info_filtered[col].astype(str))

# ê²°ì¸¡ê°’ ì²˜ë¦¬
imputer = SimpleImputer(strategy="most_frequent")
wine_info_filtered[feature_columns] = imputer.fit_transform(wine_info_filtered[feature_columns])

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ
X = wine_info_filtered[feature_columns]
y = wine_info_filtered[target_column].astype(float)
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X, y)

# ì†ì„± ì¤‘ìš”ë„ ì €ì¥
rf_importance = pd.DataFrame({'Feature': feature_columns, 'RF_Importance': model.feature_importances_})
rf_importance = rf_importance.sort_values(by='RF_Importance', ascending=False)

# ğŸ“Œ 4ï¸âƒ£ SVD ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ ì¤‘ìš”ë„ ë¶„ì„
print("ğŸ“¢ ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ì†ì„± ì¤‘ìš”ë„ ë¶„ì„ ì¤‘...")
feature_matrix = wine_info_filtered.drop(columns=[target_column]).values
cosine_sim_matrix = cosine_similarity(feature_matrix.T)

# íŠ¹ì´ê°’ ë¶„í•´(SVD) ìˆ˜í–‰
svd = TruncatedSVD(n_components=1)
svd.fit(cosine_sim_matrix)
importance_scores = np.abs(svd.components_[0])  # ì ˆëŒ“ê°’ì„ ì·¨í•´ ì¤‘ìš”ë„ ìŠ¤ì½”ì–´í™”

# ë„¤íŠ¸ì›Œí¬ ì¤‘ìš”ë„ ì €ì¥
network_importance = pd.DataFrame({"Feature": feature_columns, "Network_Importance": importance_scores})
network_importance = network_importance.sort_values(by="Network_Importance", ascending=False)

# ğŸ“Œ 5ï¸âƒ£ ë‘ ì ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬ ìµœì¢… ì¤‘ìš”ë„ ê³„ì‚°
print("ğŸ“¢ ëœë¤ í¬ë ˆìŠ¤íŠ¸ + ë„¤íŠ¸ì›Œí¬ ì ìˆ˜ ì¡°í•© ì¤‘...")
importance_df = rf_importance.merge(network_importance, on="Feature")

# ì ìˆ˜ ì •ê·œí™”
scaler = MinMaxScaler()
importance_df[["RF_Importance", "Network_Importance"]] = scaler.fit_transform(
    importance_df[["RF_Importance", "Network_Importance"]]
)

# ìµœì¢… ì ìˆ˜ = 0.5 * RF + 0.5 * ë„¤íŠ¸ì›Œí¬ (ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì • ê°€ëŠ¥)
importance_df["Final_Importance"] = (
    0.2 * importance_df["RF_Importance"] + 0.8 * importance_df["Network_Importance"]
)

# ì •ë ¬ í›„ ìµœìƒìœ„ ì†ì„± ì„ íƒ
importance_df = importance_df.sort_values(by="Final_Importance", ascending=False)
final_selected_features = importance_df.nlargest(10, "Final_Importance")["Feature"].tolist()

# ğŸ“Œ 6ï¸âƒ£ ìµœì¢… ì†ì„± ì‹œê°í™”
plt.figure(figsize=(12, 6))
sns.barplot(x=importance_df["Final_Importance"][:10], y=importance_df["Feature"][:10], palette="coolwarm")
plt.xlabel("Final Importance Score")
plt.ylabel("Feature")
plt.title("Top 10 Features by Combined Importance (RF + SVD)")
plt.show()

print(f"ğŸ¯ ìµœì¢… ì„ ì •ëœ relation ì†ì„± ëª©ë¡: {final_selected_features}")

# ğŸ“Œ 7ï¸âƒ£ ìµœì¢… ì†ì„± ì €ì¥
final_features_df = pd.DataFrame(final_selected_features, columns=["Selected Features"])
final_features_df.to_csv("wine/final_selected_features.csv", index=False)
print("âœ… ìµœì¢… ì„ ì •ëœ relation ì†ì„±ì´ 'wine/final_selected_features.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")


