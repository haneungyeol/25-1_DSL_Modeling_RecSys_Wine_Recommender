# í™˜ê²½ ì„¤ì •

!pip install sentence-transformers

!pip install torch torchvision torchaudio torch-geometric pandas scikit-learn numpy

import os
import torch
import random
import pickle
import numpy as np
import pandas as pd
import torch.nn as nn
from sklearn.preprocessing import LabelEncoder

# ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("/content/drive/MyDrive/25-1 DSL Modeling/modelling data/filtered_train_data.csv")
val_df   = pd.read_csv("/content/drive/MyDrive/25-1 DSL Modeling/modelling data/filtered_val_data.csv")
test_df  = pd.read_csv("/content/drive/MyDrive/25-1 DSL Modeling/modelling data/filtered_test_data.csv")

# ì™€ì´ë“œ í¬ë§·ì„ ë¡± í¬ë§·ìœ¼ë¡œ ë³€í™˜
def wide_to_long(df):
    df = df.rename(columns={df.columns[0]: "user"})
    melted = df.melt(id_vars="user", var_name="wine", value_name="rating")
    melted.dropna(subset=["rating"], inplace=True)
    melted.reset_index(drop=True, inplace=True)
    return melted

train_long_df = wide_to_long(train_df)
val_long_df   = wide_to_long(val_df)
test_long_df  = wide_to_long(test_df)

# ì‚¬ìš©ì ë° ì™€ì¸ ID ì¸ì½”ë”©
user_encoder = LabelEncoder()
wine_encoder = LabelEncoder()

all_users = pd.concat([train_long_df["user"], val_long_df["user"], test_long_df["user"]]).unique()
all_wines = pd.concat([train_long_df["wine"], val_long_df["wine"], test_long_df["wine"]]).unique()

user_encoder.fit(all_users)
wine_encoder.fit(all_wines)

for df in [train_long_df, val_long_df, test_long_df]:
    df["user_id"] = user_encoder.transform(df["user"])
    df["wine_id"] = wine_encoder.transform(df["wine"])

# ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (ê° ì‚¬ìš©ìë³„ 70%-30%)
def split_data(df):
    train_indices, holdout_indices = [], []
    for user in df["user"].unique():
        user_data = df[df["user"] == user]
        if len(user_data) < 2:
            train_indices.extend(user_data.index)
        else:
            chosen = np.random.choice(user_data.index, size=int(len(user_data) * 0.7), replace=False)
            holdout = list(set(user_data.index) - set(chosen))
            train_indices.extend(chosen)
            holdout_indices.extend(holdout)
    return df.loc[train_indices], df.loc[holdout_indices]

val_train_df, val_test_df = split_data(val_long_df)
test_train_df, test_test_df = split_data(test_long_df)

# MCCFì—ì„œ ìš”êµ¬í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (rating * 10 ì ìš©)
def df_to_mccf_format(df, log_transform=False, rating_scale_factor=10):
    ratings = df["rating"].astype(float)
    if log_transform:
        ratings = np.log1p(ratings) * rating_scale_factor  # log(rating + 1) * 10
    return list(zip(
        df["user_id"].astype(int),
        df["wine_id"].astype(int),
        ratings
    ))

train_data = df_to_mccf_format(train_long_df)
val_train_data = df_to_mccf_format(val_train_df)
val_test_data = df_to_mccf_format(val_test_df)
test_train_data = df_to_mccf_format(test_train_df)
test_test_data = df_to_mccf_format(test_test_df)

# ì‚¬ìš©ì ë° ì•„ì´í…œ ìˆ˜ ê³„ì‚°
num_total_users = max(train_long_df["user_id"].max(), val_long_df["user_id"].max(), test_long_df["user_id"].max()) + 1
num_total_items = max(train_long_df["wine_id"].max(), val_long_df["wine_id"].max(), test_long_df["wine_id"].max()) + 1

# ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ ìƒì„± (MCCF ë°©ì‹)
u_adj = {i: [] for i in range(num_total_users)}
i_adj = {i: [] for i in range(num_total_items)}

for user_id, wine_id, rating in train_data:
    u_adj[user_id].append((wine_id, rating))
    i_adj[wine_id].append((user_id, rating))

# ì‚¬ìš©ì ë° ì•„ì´í…œ íŠ¹ì„± ë²¡í„° ì´ˆê¸°í™”
ufeature = np.zeros((num_total_users, num_total_items), dtype=np.float32)
ifeature = np.zeros((num_total_items, num_total_users), dtype=np.float32)

for user_id, items in u_adj.items():
    for wine_id, rating in items:
        ufeature[user_id, wine_id] = rating

for wine_id, users in i_adj.items():
    for user_id, rating in users:
        ifeature[wine_id, user_id] = rating

# PyTorch ì„ë² ë”© ë³€í™˜
ufeature_tensor = torch.tensor(ufeature, dtype=torch.float32)
ifeature_tensor = torch.tensor(ifeature, dtype=torch.float32)

u2e = nn.Embedding.from_pretrained(ufeature_tensor, freeze=False)
i2e = nn.Embedding.from_pretrained(ifeature_tensor, freeze=False)

# ë°ì´í„° ì €ì¥ (MCCF ëª¨ë¸ìš©)
output_path = "mccf_data.p"
with open(output_path, "wb") as meta:
    pickle.dump((u2e, i2e, train_data, val_train_data, val_test_data, test_train_data, test_test_data, u_adj, i_adj), meta)

print(f"âœ… MCCF ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {output_path}")

import pandas as pd
import numpy as np
import torch
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer

# CSV íŒŒì¼ ë¡œë“œ
df = pd.read_csv("/content/drive/MyDrive/25-1 DSL Modeling/wine item data/Final_Merged_Wine_Data.csv")

# 1. ë§›Â·í–¥ ê´€ë ¨: Flavor Group ë° Keywords ì²˜ë¦¬
flavor_cols = ["Flavor Group 1", "Keywords 1", "Flavor Group 2", "Keywords 2", "Flavor Group 3", "Keywords 3"]
df["flavor_text"] = df[flavor_cols].fillna("").agg(" ".join, axis=1)

# 2. ìƒì‚° ë° ìŠ¤íƒ€ì¼ ê´€ë ¨: Grapes, Region, Wine style, Winery, Alcohol content ì²˜ë¦¬
prod_cols = ["Grapes", "Region", "Wine style", "Winery", "Alcohol content"]
df["prod_text"] = df[prod_cols].fillna("").agg(" ".join, axis=1)

# 3. Food Pairing ì²˜ë¦¬
df["food_pairing_text"] = df["Food Pairing"].fillna("")

# BERT ëª¨ë¸ ë¡œë“œ (ì˜ˆ: all-MiniLM-L6-v2, ì„ë² ë”© ì°¨ì› 384)
bert_model = SentenceTransformer('all-MiniLM-L6-v2')

# BERT ì„ë² ë”© ê³„ì‚° í•¨ìˆ˜
def get_bert_embeddings(text_list, target_dim=64):
    # text_list: ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ í…ìŠ¤íŠ¸ (ë¬¸ì¥)ë“¤
    embeddings = bert_model.encode(text_list, show_progress_bar=True)
    # embeddings: (num_samples, 384)
    # PCAë¡œ target_dim ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
    pca = PCA(n_components=target_dim, random_state=42)
    reduced = pca.fit_transform(embeddings)
    return reduced  # (num_samples, target_dim)

# ê° ì˜ì—­ë³„ BERT ì„ë² ë”© ê³„ì‚° (ëª©í‘œ: 64ì°¨ì›)
flavor_emb = get_bert_embeddings(df["flavor_text"].tolist(), target_dim=64)
prod_emb = get_bert_embeddings(df["prod_text"].tolist(), target_dim=64)
food_emb = get_bert_embeddings(df["food_pairing_text"].tolist(), target_dim=64)

# 4. ì‚¬ì´ë“œ ì •ë³´ í…ì„œ êµ¬ì„±
# ê° ì™€ì¸ì— ëŒ€í•´: í† í° 1 = ë§›Â·í–¥ (flavor_emb), í† í° 2 = ìƒì‚°/ìŠ¤íƒ€ì¼ (prod_emb), í† í° 3 = Food Pairing (food_emb)
wine_side_features_np = np.stack([flavor_emb, prod_emb, food_emb], axis=1)  # shape: (num_items, 3, 64)
wine_side_features = torch.tensor(wine_side_features_np, dtype=torch.float)

# MCCFì— ë§ê²Œ ë°ì´í„° ë³€í™˜
def df_to_mccf_format(df):
    return list(zip(df["user_id"].astype(int), df["wine_id"].astype(int), df["rating"].astype(float)))

train_data = df_to_mccf_format(train_long_df)
val_train_data = df_to_mccf_format(val_train_df)
val_test_data = df_to_mccf_format(val_test_df)
test_train_data = df_to_mccf_format(test_train_df)
test_test_data = df_to_mccf_format(test_test_df)

num_total_users = train_long_df["user_id"].nunique()
num_total_items = train_long_df["wine_id"].nunique()

# âœ… (num_items, 3, 64) â†’ (num_items, 192)
wine_side_features_reshaped = wine_side_features.view(wine_side_features.shape[0], -1)
#print("âœ… Reshaped wine_side_features:", wine_side_features_reshaped.shape)

# PyTorch Embedding ì—…ë°ì´íŠ¸ (ì•„ì´í…œ ì •ë³´ ë°˜ì˜)
i2e = nn.Embedding.from_pretrained(wine_side_features_reshaped, freeze=False)

# ë°ì´í„° ì €ì¥ (MCCF ëª¨ë¸ìš©)
output_path = "/content/drive/MyDrive/25-1 DSL Modeling/mccf_data.p"
with open(output_path, "wb") as meta:
    pickle.dump((i2e, train_data, val_train_data, val_test_data, test_train_data, test_test_data), meta)

print(f"âœ… MCCF ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {output_path}")

print("wine_side_features shape:", wine_side_features.shape)

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.nn.functional as F

class MCCF(nn.Module):
    def __init__(self, user_embedding, item_embedding, embed_dim, wine_side_info=None, N=30000, dropout_rate=0.5, beta_ema=0.999):
        super(MCCF, self).__init__()

        self.user_embedding = user_embedding
        self.item_embedding = item_embedding
        self.embed_dim = embed_dim
        self.N = N
        self.dropout_rate = dropout_rate
        self.beta_ema = beta_ema
        self.criterion = nn.MSELoss()

        # âœ… ì™€ì¸ ë¶€ê°€ ì •ë³´ ì¶”ê°€ (ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° ë³€í™˜)
        if wine_side_info is not None:
            if isinstance(wine_side_info, list):
                wine_side_info = torch.tensor(wine_side_info, dtype=torch.float32)
            self.wine_side_info = wine_side_info
            wine_feature_dim = self.wine_side_info.shape[1]
        else:
            self.wine_side_info = None
            wine_feature_dim = 0

        print(f"âœ… ì™€ì¸ ë¶€ê°€ ì •ë³´ ì‚¬ìš© ì—¬ë¶€: {'Yes' if wine_feature_dim > 0 else 'No'} | ì°¨ì›: {wine_feature_dim}")
        #print(f"âœ… ì•„ì´í…œ ì„ë² ë”© ì°¨ì›: {self.item_embedding.embedding_dim}")

        # âœ… ì„ë² ë”© ì°¨ì›(469) + wine ë¶€ê°€ ì •ë³´ ì°¨ì›(3) = ì´ ì…ë ¥ ì°¨ì›
        item_input_dim = self.item_embedding.embedding_dim + wine_feature_dim
        self.item_layer1 = nn.Linear(item_input_dim, self.embed_dim)
        self.item_layer2 = nn.Linear(self.embed_dim, self.embed_dim)

        # ì‚¬ìš©ì-ì•„ì´í…œ ìƒí˜¸ì‘ìš© í•™ìŠµ MLP
        self.interaction_layer1 = nn.Linear(self.embed_dim * 2, self.embed_dim)
        self.interaction_layer2 = nn.Linear(self.embed_dim, 1)



        #print("ğŸ” item_embedding.embedding_dim:", self.item_embedding.embedding_dim)


    def forward(self, user_ids, item_ids, wine_features=None):
        #print("â–¶ï¸ forward() í˜¸ì¶œë¨")  # âœ… ì œì¼ ì•ì— ìœ„ì¹˜

        user_embedded = self.user_embedding(user_ids)
        item_embedded = self.item_embedding(item_ids)

        if self.wine_side_info is not None and wine_features is None:
            wine_features = self.wine_side_info[item_ids]

        if wine_features is not None:
            #print(f"[Debug] wine_features shape: {wine_features.shape}")
            item_embedded = torch.cat((item_embedded, wine_features), dim=1)

        #print(f"[Debug] item_embedded shape: {item_embedded.shape}")

        item_hidden = F.relu(self.item_layer1(item_embedded))
        item_hidden = self.item_layer2(item_hidden)

        interaction_input = torch.cat((user_embedded, item_hidden), dim=1)
        #print(f"ğŸ§© interaction_input shape: {interaction_input.shape}")
        scores = self.interaction_layer2(F.relu(self.interaction_layer1(interaction_input)))


        #print(f"ğŸ“¦ item_embedded shape before concat: {item_embedded.shape}")


        return scores.squeeze()  # âœ… returnì€ ë§ˆì§€ë§‰ì—


    def compute_loss(self, user_ids, item_ids, ratings, wine_features=None):
        """
        ëª¨ë¸ ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°

        Args:
        - user_ids (torch.Tensor): ì‚¬ìš©ì ID í…ì„œ
        - item_ids (torch.Tensor): ì•„ì´í…œ(ì™€ì¸) ID í…ì„œ
        - ratings (torch.Tensor): ì‹¤ì œ í‰ì  í…ì„œ
        - wine_features (torch.Tensor, optional): ì•„ì´í…œì˜ ë¶€ê°€ì  ì™€ì¸ ì •ë³´

        Returns:
        - loss (torch.Tensor): MSE ì†ì‹¤ ê°’
        """
        predicted_scores = self.forward(user_ids, item_ids, wine_features)
        loss = self.criterion(predicted_scores, ratings)
        return loss

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error

def test(model, test_loader, device):
    """
    MCCF ëª¨ë¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (RMSE, MAE í‰ê°€)

    Args:
    - model (MCCF): í•™ìŠµëœ MCCF ëª¨ë¸
    - test_loader (DataLoader): í…ŒìŠ¤íŠ¸ ë°ì´í„° ë°°ì¹˜ ë¡œë”
    - device (torch.device): í•™ìŠµ ì¥ì¹˜ (GPU/CPU)

    Returns:
    - rmse (float): Root Mean Squared Error (RMSE)
    - mae (float): Mean Absolute Error (MAE)
    """
    model.eval()
    pred, ground_truth = [], []

    with torch.no_grad():  # âœ… No gradient computation for inference
        for test_u, test_i, test_ratings, test_wine_features in test_loader:
            # âœ… ë°ì´í„° GPU/CPUë¡œ ì´ë™
            test_u = test_u.to(device)
            test_i = test_i.to(device)
            test_ratings = test_ratings.to(device)
            test_wine_features = test_wine_features.to(device) if test_wine_features is not None else None

            # âœ… ì˜ˆì¸¡ ìˆ˜í–‰
            scores = model(test_u, test_i)

            # âœ… NumPy ë³€í™˜ í›„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            pred.append(scores.cpu().numpy())
            ground_truth.append(test_ratings.cpu().numpy())

    # âœ… ë¦¬ìŠ¤íŠ¸ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
    pred = np.concatenate(pred)
    ground_truth = np.concatenate(ground_truth)

    # âœ… RMSE & MAE ê³„ì‚°
    rmse = mean_squared_error(ground_truth, pred)  # âœ… squared=False ì˜µì…˜ ì‚¬ìš©
    mae = mean_absolute_error(ground_truth, pred)

    print(f"âœ… Test ê²°ê³¼ - RMSE: {rmse:.5f}, MAE: {mae:.5f}")
    return rmse, mae

import numpy as np
import torch

def recall_at_k(y_true, y_pred, k=20):
    y_pred = y_pred[:k]
    num_relevant = len(set(y_true) & set(y_pred))
    return num_relevant / len(y_true) if len(y_true) > 0 else 0

def precision_at_k(y_true, y_pred, k=20):
    y_pred = y_pred[:k]
    num_relevant = len(set(y_true) & set(y_pred))
    return num_relevant / k

def ndcg_at_k(y_true, y_pred, k=20):
    y_pred = y_pred[:k]
    dcg = 0.0
    for idx, item in enumerate(y_pred):
        if item in y_true:
            dcg += 1 / np.log2(idx + 2)  # log2(rank + 1)
    idcg = sum(1 / np.log2(i + 2) for i in range(min(len(y_true), k)))
    return dcg / idcg if idcg > 0 else 0

def evaluate_mccf(model, test_data, k=20):
    recall_list, ndcg_list, precision_list = [], [], []

    users = list(set([u for u, _, _ in test_data]))

    for user in users:
        user_actual_items = set([i for u, i, r in test_data if u == user and r > 0])

        with torch.no_grad():
            item_scores = {i: model.forward(torch.tensor([user]), torch.tensor([i])).item()
                           for _, i, _ in test_data}

        sorted_items = sorted(item_scores, key=item_scores.get, reverse=True)[:k]

        recall_list.append(recall_at_k(user_actual_items, sorted_items, k))
        precision_list.append(precision_at_k(user_actual_items, sorted_items, k))
        ndcg_list.append(ndcg_at_k(user_actual_items, sorted_items, k))

    recall_avg = np.mean(recall_list)
    precision_avg = np.mean(precision_list)
    ndcg_avg = np.mean(ndcg_list)

    print(f"\U0001F4CA Evaluation Results - Recall@{k}: {recall_avg:.5f}, Precision@{k}: {precision_avg:.5f}, NDCG@{k}: {ndcg_avg:.5f}")
    return recall_avg, precision_avg, ndcg_avg

import argparse
from torch.utils.data import TensorDataset

def main():
    import argparse
    import pickle
    import torch
    import numpy as np
    from torch.utils.data import TensorDataset, DataLoader

    parser = argparse.ArgumentParser(description="MCCF")
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--lr', type=float, default=0.001)
    parser.add_argument('--embed_dim', type=int, default=64)
    parser.add_argument('--batch_size', type=int, default=256)
    parser.add_argument('--test_batch_size', type=int, default=256)
    parser.add_argument('--droprate', type=float, default=0.3)

    args, unknown = parser.parse_known_args()
    print('-------------------- Hyperparams --------------------')
    print(f"Learning rate: {args.lr}, Embedding Dimension: {args.embed_dim}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # âœ… Load data
    dataset_path = '/content/mccf_data.p'
    with open(dataset_path, "rb") as f:
        loaded_data = pickle.load(f)

    if len(loaded_data) < 5:
        raise ValueError("âŒ ë°ì´í„°ì…‹ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ìµœì†Œ 5ê°œì˜ ìš”ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    user_embedding, item_embedding, train_data, test_data, wine_side_info = loaded_data[:5]
    print(f"âœ… Train ë°ì´í„° í¬ê¸°: {len(train_data)}, Test ë°ì´í„° í¬ê¸°: {len(test_data)}")

    # âœ… user embedding ì°¨ì› ì¤„ì´ê¸° (64ì°¨ì›ë§Œ ì‚¬ìš©)
    user_embedding_matrix = user_embedding.weight.data[:, :args.embed_dim]
    user_embedding = nn.Embedding.from_pretrained(user_embedding_matrix.clone(), freeze=False)

    item_embedding_matrix = item_embedding.weight.data[:, :args.embed_dim]
    item_embedding = nn.Embedding.from_pretrained(item_embedding_matrix.clone(), freeze=False)

    print("âœ… user_embedding type:", type(user_embedding))

    if isinstance(wine_side_info, torch.Tensor) and wine_side_info.dim() == 3:
        wine_side_info = wine_side_info.view(wine_side_info.shape[0], -1)

    wine_side_info = torch.FloatTensor(wine_side_info)
    num_items = wine_side_info.shape[0]

    # âœ… ì „ì²˜ë¦¬: ìœ íš¨í•œ item ì¸ë±ìŠ¤ë§Œ ìœ ì§€
    train_data = [x for x in train_data if x[1] < num_items]
    test_data = [x for x in test_data if x[1] < num_items]

    # ğŸ” ë‹¤ì‹œ ì¸ë±ìŠ¤ ì¶”ì¶œ
    item_indices_train = torch.LongTensor([x[1] for x in train_data])
    item_indices_test = torch.LongTensor([x[1] for x in test_data])

    trainset = TensorDataset(
        torch.LongTensor([x[0] for x in train_data]),
        torch.LongTensor([x[1] for x in train_data]),
        torch.FloatTensor([x[2] for x in train_data]),
        wine_side_info.index_select(0, item_indices_train)
    )

    testset = TensorDataset(
        torch.LongTensor([x[0] for x in test_data]),
        torch.LongTensor([x[1] for x in test_data]),
        torch.FloatTensor([x[2] for x in test_data]),
        wine_side_info.index_select(0, item_indices_test)
    )

    train_loader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=4)
    test_loader = DataLoader(testset, batch_size=args.test_batch_size, shuffle=False, num_workers=4)

    # âœ… ëª¨ë¸ ì´ˆê¸°í™”
    model = MCCF(
        user_embedding,
        item_embedding,
        args.embed_dim,
        wine_side_info=wine_side_info,
        dropout_rate=args.droprate
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    best_rmse, best_mae = np.inf, np.inf
    endure_count = 0

    for epoch in range(1, args.epochs + 1):
        train(model, train_loader, optimizer, epoch, device)
        rmse, mae = test(model, test_loader, device)

        print(f"<Test> RMSE: {rmse:.5f}, MAE: {mae:.5f}")

        if endure_count > 30:
            break

    # âœ… Top-K í‰ê°€
    print("ğŸ” Running Top-K Evaluation...")
    recall, precision, ndcg = evaluate_mccf(model, test_data, k=20)

    print(f"Best RMSE/MAE: {best_rmse:.5f} / {best_mae:.5f}")
    print(f"Top-K Metrics â€” Recall@20: {recall:.5f}, Precision@20: {precision:.5f}, NDCG@20: {ndcg:.5f}")

